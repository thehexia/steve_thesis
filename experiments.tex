\chapter{Experiments} \label{ch:experiments}

%In order to validate that Steve applications are not incredibly slow, performance experiments done on compiled Steve applications. 

This thesis is not performance based. However, to verify some questions and hypotheses about elements of the language design, some performance experiments were performed.
These experiments were performed using \textit{Fakeflow}, a Freeflow data plane emulator. Fakeflow is largely equivalent to a single-threaded implementation of Freeflow with the exception of ports. Fakeflow removes the overhead of sending and receiving from ports. Instead, Fakeflow reads packets from pcap files to emulate receiving from ports and forwarded packets do not get sent anywhere (equivalent to simply dropping the packet). This approach is taken because the speed at which the data plane outputs packets varies largely with implementation. The goal of this chapter is to focus only on the performance of the application, not the speed of switch resources.
Specifically, two questions are evaluated.

\begin{enumerate}
%\item How fast can these applications process packets on a general purpose CPU? Comparison of these numbers against a real switch is excluded because it is unfair. Real programmable switches have specialized hardware to speed up table matching and decoding. Instead, these numbers are used to compare how certain use cases and packet structures affect a Steve application's performance.

\item Does partial decodes produce measurable gains over full decodes? Steve was designed to support partial decodes, so for the design choice to be valid, this must be true.

\item What is the performance of each individual action or operation on a general purpose CPU? Knowing this allows the programmer to make certain optimization choices when writing code.
\end{enumerate}

\section{Data Sets} \label{exp:use_cases}

Tests were performed using three pcap samples described in Table \ref{tbl:pcap}. 
The samples are relatively diverse, having varying packet sizes, packet count, and protocols. Samples 1 and 2 were taken from Tcpreplay's sample captures \footnote{http://tcpreplay.appneta.com/wiki/captures.html} used for testing NetFlow. Sample 3 was taken from Netresec \footnote{http://www.netresec.com/?page=PCAP4SICS} which contains real industrial network traffic passing through ICS labs.

\begin{table}[ht]
\caption{The three pcap test cases used for experiments.}
\begin{center}
\begin{tabularx}{\linewidth}{| c || c | c | c | X |}
\hline
\# & Pcap sample & Packet Count & Average Packet Size & Summary \\
\hline
1 & smallFlows & 14,261 & 646 bytes & A synthetic capture using various different protocols. \\
\hline
2 & bigFlows & 791,615 & 449 bytes & Real network traffic from a busy private network's Internet access point. \\
\hline
3 & 4SICS & 2,274,747 & 76 bytes & Real industrial network traffic from ICS labs. \\
\hline
\end{tabularx}
\end{center}
\label{tbl:pcap}
\end{table}


%\section{Use Cases}
%
%This section provides experimental performance results for four applications from Appendix \ref{ap:steve_programs}: the MAC learning switch, the IPv4 learning switch, the basic wire, and the basic packet filter/stateless firewall. The timeout property for flow entries in all examples were removed so flow entries last indefinitely. Tests were run on an Intel(R) Core(TM) i3-2130 CPU with a speed of 3.40GHz.
%
%The Freeflow emulator was configured to have a fixed number of ports. The ingress port for each packet was randomly assigned amongst these ports. For each experiment, there were five ingress ports (except for the wire which naturally has two ports). 
%
%Table \ref{tbl:pcap1_stats} through Table \ref{tbl:pcap3_stats} summarize the results of running the same pcap sample a certain number of iterations through each application. The number of iterations chosen for each sample are such that approximately 90 million packets of each sample are sent through each use case. Packets are sent through the pipeline at maximum rate rather than at their original rate. Each experiment is repeated five times and the resulting averages for each measure are given.
%
%Three measures of performance are used. First, the amount of data processed per second, known as \textit{data rate}, is given as gigabits per second (Gbps). Second, the amount of data (that would be) forwarded per second, known as \textit{throughput}, is given as Gbps. Throughput is always less than or equal to data rate. Lastly, the amount of packets processed per second (ignoring the size of data in those packets), called \textit{packet processing rate}, is given in millions-of-packets-per-second (Mpps).
%
%% % % % % % % % % %
%% smallFlows
%% % % % % % % % % %
%\begin{table}
%\caption{Performance metrics after sending 6380 iterations of the smallFlows pcap sample through each application.}
%\begin{center}
%\begin{tabularx}{0.9\textwidth}{| c || c | c | c |}
%\hline
%% header row
%Application & Data Rate (Gbps) & Throughput (Gbps) & Packet Rate (Mpps)  \\
%\hline
%MAC Learning & 10.10 & 10.10 & 1.96  \\
%\hline
%IPv4 Learning & 8.15 & 8.15 & 1.58  \\
%\hline 
%Wire* & 16.41 & 16.41 & 3.17 \\
%\hline
%Firewall & 8.47 & 0.94 & 1.64 \\
%\hline
%\multicolumn{4}{p{0.9\textwidth}}{* Two dummy packets are sent from both ports on the wire to ensure the application learns those ports before processing packets.}
%\end{tabularx}
%\end{center}
%\label{tbl:pcap1_stats}
%\end{table}
%
%
%Table \ref{tbl:pcap1_stats} shows the performance of each application using Sample 1. Results here are largely predictable. 
%In terms of performance, applications which process less headers perform better than those that process more headers. The amount of tables also has a large impact as table matching is one of the more expensive operations (see Table \ref{tbl:action_stats}). The wire application performs significantly better because it performs no table matching.
%Similarly, applications which do less flow entry inserting perform significantly better than those that do. The firewall example, for instance, is an extension of the IPv4 switch, yet processes packets faster. This is because only approximately 83\% of packets in Sample 1 used port 80 or 443, so 17\% of packets get prematurely dropped, reducing the amount of learning done.
%Table \ref{tbl:pcap2_stats}, which uses Sample 2, largely shows the same patterns. 
%The difference in measures between firewall and IPv4 switch is more exaggerated with this sample because only 51.94\% of packets used port 80 or 443.
%
%% % % % % % % % % %
%% Big flows
%% % % % % % % % % %
%\begin{table}
%\caption{Performance metrics after sending 114 iterations of the bigFlows pcap sample through each application.}
%\begin{center}
%\begin{tabularx}{0.9\textwidth}{| c || c | c | c | }
%\hline
%Application & Data Rate (Gbps) & Throughput (Gbps) & Packet Rate (Mpps)  \\
%\hline
%MAC Learning & 7.07 & 7.07 & 1.97  \\
%\hline
%IPv4 Learning & 5.77 & 5.77 & 1.61  \\
%\hline 
%Wire* & 12.07 & 12.07 & 3.36 \\
%\hline
%Firewall & 6.29 & 1.35 & 1.75 \\
%\hline
%\multicolumn{4}{p{0.9\textwidth}}{* Two dummy packets are sent from both ports on the wire to ensure the application learns those ports before processing packets.}
%\end{tabularx}
%\end{center}
%\label{tbl:pcap2_stats}
%\end{table}
%
%
%The relationship between data rate, packet rate, and packet size is of interest here.
%The lower the packet size, the lower the data rate. The time it takes to process a packet is more strongly affected by the number of headers processed and actions taken. The size of the packet has comparatively small impact.
%This is because payloads, which typically make up most of a packet's size, are disregarded by the application.
%
%Packet sizes in Sample 2 are on average 197 bytes (or about 30.50\%) smaller than Sample 1. The data rate for from Table \ref{tbl:pcap1_stats} to Table \ref{tbl:pcap2_stats} is approximately 27.86\% slower. This roughly corresponds to the decrease in packet sizes.
%
%However, the lower the packet size, the higher the packet processing rate.
%This is explained by the amount of time it takes to read and save packets into program memory.
%Smaller packets mean slight improvements in packet rate. However, this improvement is small when compared to loses in data rate.
%The largest increase in packet rate is 6.00\% with wire. The conclusion can be drawn that the benefit of larger packets producing higher data rates far outweigh the loss in packet rate.
%
%Table \ref{tbl:pcap3_stats} uses Sample 3, which has a significantly smaller average packet size than other samples. 
%Packet processing rates for all applications significantly improved here due to the smaller packet size. However, data rates and throughput severely suffered. 
%When compared against Sample 1, Sample 3 is 87.77\% smaller. On average, data rates were 86.34\% slower when Sample 3 was run through compared to Sample 1.
%The average increase in packet processing rate was only 16.12\%. This reinforces that data rate grows faster with packet size than packet processing rates from smaller packet sizes.
%
%% % % % % % % % % %
%% 4SICS flows
%% % % % % % % % % %
%\begin{table}[ht]
%\caption{Performance metrics after sending 20 iterations of the 4SICS pcap sample through each application.}
%\begin{center}
%\begin{tabularx}{\linewidth}{| c || c | c | c | }
%\hline
%Application & Data Rate (Gbps) & Throughput (Gbps) & Packet Rate (Mpps) \\
%\hline
%MAC Learning & 1.30 & 1.30 & 2.14  \\
%\hline
%IPv4 Learning & 1.07 & 1.06 & 1.76  \\
%\hline 
%Wire* & 2.28 & 2.28 & 3.75 \\
%\hline
%Firewall & 1.25 & 0.04 & 2.06 \\
%\hline
%\multicolumn{4}{p{\linewidth}}{* Two dummy packets are sent from both ports on the wire to ensure the application learns those ports before processing packets.}
%\end{tabularx}
%\end{center}
%\label{tbl:pcap3_stats}
%\end{table}

\section{Partial vs. Full Decodes} \label{exp:decode_comparison}

Steve proposed that partial decodes produced measurable gains over full decodes. To demonstrate this, an experiment was run using the stateless firewall application. The original version had partial decodes of ethernet, TCP, and UDP headers. A duplicate was written with full decodes of all headers.

%Each sample from Table \ref{tbl:pcap} is run through the full decoding firewall the same number of iterations as described in Tables \ref{tbl:pcap1_stats} through \ref{tbl:pcap3_stats}. These experiments are run five times each and an average is recorded. Results are compared against the partial decoding firewall.
Each sample is run through a certain number of iterations, so that approximately 90 million packets from each sample are sent through.
The timeout property for flow entries in all examples were removed so flow entries last indefinitely. Tests were run on an Intel(R) Core(TM) i3-2130 CPU with a speed of 3.40GHz.
The Freeflow emulator was configured to have a fixed number of ports. The ingress port for each packet was randomly assigned amongst these ports. For each experiment, there were five ingress ports. 
Table \ref{tbl:firewall_cmp} summarizes these results. The table also includes the percentage of TCP and UDP packets per sample. The full decoding firewall extracts four additional fields on UDP packets and nine additional fields on TCP packets. 

\begin{table}[ht]
\caption{Comparing firewall performance (in Mpps) with partial header decodes versus full header decodes.}
\begin{center}
\begin{tabularx}{\linewidth}{| c || c | c | c | c | c | X | c |}
\hline
Sample & Iter. & Partial (Mpps) & Full (Mpps) & \% TCP & \% UDP & Approx. Difference (\# packets) & Percentage Diff. \\
\hline
1 & 6380 & 1.64 & 1.56 & 96.12\% & 3.67\% & 80,000 & 5.00\% \\
\hline
2 & 114 & 1.75 & 1.63 & 80.22\% & 19.45\% & 120,000 & 7.10\% \\
\hline
3 & 40 & 2.06 & 1.91 & 95.25\% & 3.68\% & 150,000 & 7.56\% \\ 
\hline
\end{tabularx}
\end{center}
\label{tbl:firewall_cmp}
\end{table}


A partial decode produces a non-negligible difference in performance with the average percentage difference between the three samples being about 6.55\%. This is only for a simple three header packet. For more complex header structures, gains from partial decodes would become more and more substantial.


\section{Performance of Operations} \label{exp:action_performance}

This section presents the typical performance of certain common operations performed by a Steve pipeline. Each operation is executed approximately 10 million times per experiment. This experiment is repeated ten times and the timings are given as an average. Table \ref{tbl:action_stats} summarizes the timed performance of each action. The output action is intentionally excluded as the results vary based on a number of factors including Freeflow implementation, threading models, and specialized forwarding hardware.

\begin{table}[ht]
\caption{Average wall clock time for executing certain operations. Output action has been excluded as it varies with the threading model implementation of Freeflow.}
\begin{center}
\begin{tabularx}{\linewidth}{| X || c | }
\hline
Operation & Time (nanoseconds)  \\
\hline
Goto action + Table matching & 110.49 \\
\hline
Insert flow entry action & 418.66 \\
\hline
Remove flow entry action & 273.53 \\
\hline
Field decode & 23.22 \\
\hline
Field access / read & 35.02 \\
\hline
Field write / set action & 29.34 \\
\hline
Write set action &  140.76 \\
\hline
Write output action & 108.92 \\
\hline
\end{tabularx}
\end{center}
\label{tbl:action_stats}
\end{table}

To reiterate the conclusion of Section \ref{exp:decode_comparison}, the average time it takes to decode a field is about 23 nanoseconds (ns). This translates to approximately 0.23 seconds of processing time per extra field extracted every 10 million packets. This is a non-negligible difference as many networking devices do expect such heavy traffic.

As mentioned in earlier sections, inserting and removing flow entries are the most expensive operations a pipeline can perform. They do notably bottleneck the pipeline and are best performed asynchronously.
It is also worth noting that writing actions to an action list is more expensive than one might anticipate (being slightly slower than even table matching). This may be a result of the current implementation dynamically allocating memory to store the action.

The goto action (which also invokes table matching) is also relatively expensive compared to other operations. It is for this reason that users should strive to reduce table matches as much as possible. The developers of POF came to a similar conclusion \cite{pof_impl}. They suggest using as few tables as possible. Multiple tables can be combined into a single table. When appropriate, a user should also consider replacing a table with a conditional statement.
