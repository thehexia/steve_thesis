\chapter{Experiments} \label{ch:experiments}

This chapter summarizes performance experiments performed on Steve compiled pipelines. These experiments were performed using \textit{Fakeflow}, a Flowpath data plane emulator. Fakeflow is largely equivalent to a single-threaded implementation of Flowpath with the exception of ports. Fakeflow removes the overhead of sending and receiving from ports. Instead, Fakeflow reads packets from pcap files to emulate receiving from ports and forwarded packets do not get sent anywhere (equivalent to simply dropping the packet).

\section{Data Sets} \label{exp:use_cases}

Tests were performed using three pcap samples described in Table \ref{tbl:pcap}. 
The samples are relatively diverse, having varying packet sizes, packet count, and protocols. Samples 1 and 2 were taken from Tcpreplay's sample captures \footnote{http://tcpreplay.appneta.com/wiki/captures.html} used for testing NetFlow. Sample 3 was taken from Netresec \footnote{http://www.netresec.com/?page=PCAP4SICS} which contains real industrial network traffic.

\begin{table}
\caption{The three pcap test cases used for experiments.}
\begin{center}
\begin{tabularx}{\linewidth}{| c || c | c | c | X |}
\hline
\# & Pcap sample & Packet Count & Average Packet Size & Summary \\
\hline
1 & smallFlows & 14,261 & 646 bytes & A synthetic capture using various different protocols. \\
\hline
2 & bigFlows & 791,615 & 449 bytes & Real network traffic from a busy private network's Internet access point. \\
\hline
3 & 4SICS & 2,274,747 & 76 bytes & Real industrial network traffic from ICS labs. \\
\hline
\end{tabularx}
\end{center}
\label{tbl:pcap}
\end{table}


\section{Use Cases}

This section provides experimental performance results for four applications from Appendix \ref{ap:steve_programs}: the MAC learning switch, the IPv4 learning switch, the basic wire, and the basic packet filter/stateless firewall. The timeout property for flow entries in all examples were removed so flow entries last indefinitely.

The Flowpath emulator was configured to have a fixed number of ports. The ingress port for each packet was randomly assigned amongst these ports. There were five ports configured (except for the wire which naturally has two ports). Table \ref{tbl:pcap1_stats} through Table \ref{tbl:pcap3_stats} summarize the results of running the same pcap sample a certain number of iterations through each application. Packets are sent through the pipeline at maximum rate rather than at their original rate. Each experiment is repeated five times and the resulting average is given.

Three measures of performance are used. First, the amount of data processed per second, known as \textit{data speed}, is given as gigabits per second (Gbps). Second, the amount of data forwarded per second, known as \textit{throughput}, is given as Gbps. Throughput is always less than or equal to processing speed. Lastly, the amount of packets processed per second (ignoring the size of data in those packets), called \textit{packet processing speed}, is given in millions-of-packets-per-second (Mpps).

% % % % % % % % % %
% smallFlows
% % % % % % % % % %
\begin{table}
\caption{Performance metrics after sending 1000 iterations of the smallFlows pcap sample through each application.}
\begin{center}
\begin{tabularx}{\linewidth}{| c || c | c | c | }
\hline
% header row
Application & Data Speed (Gbps) & Throughput (Gbps) & Processing Speed (Mpps)  \\
\hline
MAC Learning & 10.10 & 10.10 & 1.95  \\
\hline
IPv4 Learning & 7.99 & 7.99 & 1.55  \\
\hline 
Wire* & 16.36 & 18.36 & 3.17 \\
\hline
Firewall & 8.82 & 0.97 & 1.71 \\
\hline
\multicolumn{4}{p{\linewidth}}{* Two dummy packets are sent from both ports on the wire to ensure the application learns those ports before processing packets.}
\end{tabularx}
\end{center}
\label{tbl:pcap1_stats}
\end{table}


Table \ref{tbl:pcap1_stats} shows the performance of each application using Sample 1. Sample 1 has the largest average packet size. Results here are largely predictable. The wire application is fastest in terms of data speed, having no headers or fields to process. The MAC learning switch performs the next fastest, having to only process the ethernet header. The IPv4 learning application is naturally slower as it must process the ethernet and IPv4 headers.

The most interesting thing to note from Table \ref{tbl:pcap1_stats} is that the firewall application, which is both an IPv4 switch and a packet filter, processes packets \textit{faster} than the IPv4 switch. However, the firewall's throughput is significantly lower. This is because packet's which do not match the filtering rules get prematurely dropped. This reduces the overall time the application spends learning (i.e. inserting flow entries) and forwarding. Learning is of course, the most expensive operation a pipeline can perform.

% % % % % % % % % %
% Big flows
% % % % % % % % % %
\begin{table}
\caption{Performance metrics after sending 20 iterations of the bigFlows pcap sample through each application.}
\begin{center}
\begin{tabularx}{\linewidth}{| c || c | c | c | }
\hline
Application & Data Speed (Gbps) & Throughput (Gbps) & Processing Speed (Mpps)  \\
\hline
MAC Learning & 6.89 & 6.89 & 1.91  \\
\hline
IPv4 Learning & 5.44 & 5.44 & 1.51  \\
\hline 
Wire* & 11.42 & 11.42 & 3.18 \\
\hline
Firewall & 6.27 & 1.35 & 1.75 \\
\hline
\multicolumn{4}{p{\linewidth}}{* Two dummy packets are sent from both ports on the wire to ensure the application learns those ports before processing packets.}
\end{tabularx}
\end{center}
\label{tbl:pcap2_stats}
\end{table}

Table \ref{tbl:pcap2_stats}, which uses Sample 2, largely shows the same patterns. One thing to note is the amount of \textit{data} processed per second is \textit{lower}, yet the difference in \textit{packets} processed per second negligible when compared to Table \ref{tbl:pcap1_stats}. This is to be expected. Packet sizes in Sample 2 are on average 197 bytes smaller than Sample 1, a difference of about 35.98\%. The time it takes to process a packet is bound by the number of headers processed and actions taken on a given packet. Payloads, which typically make up most of a packet's size, are disregarded. Thus, the length of the packet does not severely affect how many packets can get processed per second. 

Larger payloads can produce a linear increase to data speeds without severely affecting packet processing speeds. The average difference in data speed between applications in Table \ref{tbl:pcap1_stats} and Table \ref{tbl:pcap2_stats} is about 36.28\%. This roughly corresponds to the difference in packet sizes.

That is not to say that the size of the packet has no affect. The amount of time it takes to allocate the buffer used to store a packet has a cost. Table \ref{tbl:pcap3_stats} uses Sample 3, which has a significantly smaller average packet size. Packet processing speeds for all applications improved here due to the smaller packet size. However, data speeds and throughput severely suffered.

% % % % % % % % % %
% 4SICS flows
% % % % % % % % % %
\begin{table}[ht]
\caption{Performance metrics after sending 20 iterations of the 4SICS pcap sample through each application.}
\begin{center}
\begin{tabularx}{\linewidth}{| c || c | c | c | }
\hline
Application & Data Speed (Gbps) & Throughput (Gbps) & Processing Speed (Mpps) \\
\hline
MAC Learning & 1.28 & 1.28 & 2.11  \\
\hline
IPv4 Learning & 1.02 & 1.01 & 1.67  \\
\hline 
Wire* & 2.29 & 2.29 & 3.77 \\
\hline
Firewall & 1.29 & 0.04 & 2.12 \\
\hline
\multicolumn{4}{p{\linewidth}}{* Two dummy packets are sent from both ports on the wire to ensure the application learns those ports before processing packets.}
\end{tabularx}
\end{center}
\label{tbl:pcap3_stats}
\end{table}


\section{Partial vs. Full Decodes} \label{exp:decode_comparison}

Steve proposed that partial decodes produced measurable gains over full decodes. To demonstrate this, an experiment was run using the stateless firewall application. The original version had partial decodes of ethernet, TCP, and UDP headers. A duplicate was written with full decodes of all headers.

Each sample from Table \ref{tbl:pcap} is run through the full decoding application the same number of iterations as described in Tables \ref{tbl:pcap1_stats} through \ref{tbl:pcap3_stats}. These experiments are run five times each and an average is recorded. Results are compared against the partial decoding applications.

Table \ref{tbl:firewall_cmp} summarizes the difference between partial and full decodes for the firewall application. The table also includes the percentage of TCP and UDP packets. The full decoding application extracts four additional fields on UDP packets and nine additional fields on TCP packets. 

\begin{table}
\caption{Comparing firewall performance (in Mpps) with partial header decodes versus full header decodes.}
\begin{center}
\begin{tabularx}{\linewidth}{| c || c | c | c | c | X | c |}
\hline
Sample & Partial (Mpps) & Full (Mpps) & \% TCP & \% UDP & Approx. Difference (\# packets) & \% Difference \\
\hline
1 & 1.71 & 1.56 & 96.12\% & 3.67\% & 150,000 & 9.17\% \\
\hline
2 & 1.75 & 1.60 & 80.22\% & 19.45\% & 150,000 & 8.96\% \\
\hline
3 & 2.12 & 1.90 & 95.25\% & 3.68\% & 220,000 & 10.95\% \\ 
\hline
\end{tabularx}
\end{center}
\label{tbl:firewall_cmp}
\end{table}


Table \ref{tbl:firewall_cmp} shows that partial decode produces a non-negligible difference in performance with the average improvement between the three samples being about 9.7\%. This is only for a simple three header packet. For more complex header structures, gains from partial decodes would become more and more substantial.


\section{Performance of Operations} \label{exp:action_performance}

This section presents the typical performance of certain common operations performed by a Steve pipeline. Each operation is executed approximately 10 million times. This experiment is repeated ten times and the timings are given as an average. Table \ref{tbl:action_stats} summarizes the time performance of each action. The output action is intentionally excluded as the results vary based on a number of factors including Flowpath implementation, driver threading models, and specialized forwarding hardware.

\begin{table}[ht]
\caption{Average wall clock time for executing certain operations. Output action has been excluded as it varies with the threading model implementation of Flowpath.}
\begin{center}
\begin{tabularx}{\linewidth}{| X || c | }
\hline
Operation & Time (nanoseconds)  \\
\hline
Goto action + Table matching & 110.49 \\
\hline
Insert flow entry action & 418.66 \\
\hline
Remove flow entry action & 273.53 \\
\hline
Field decode & 23.22 \\
\hline
Field access / read & 35.02 \\
\hline
Field write / set action & 29.34 \\
\hline
Write set action &  140.76 \\
\hline
Write output action & 108.92 \\
\hline
\end{tabularx}
\end{center}
\label{tbl:action_stats}
\end{table}

To reiterate the conclusion of Section \ref{exp:decode_comparison}, the average time it takes to decode a field is about 23 nanoseconds (ns). This translates to approximately 0.23 seconds of processing time per extra field extracted every 10 million packets.

As mentioned in earlier sections, inserting and removing flow entries are the most expensive operations a pipeline can perform. It is also worth noting that writing actions to an action set is more expensive than one might anticipate (being slightly slower than even table matching).

The goto action (which also invokes table matching) is also relatively expensive compared to other operations. It is for this reason that users should strive to reduce the number of table matches as much as possible. The developers of POF \cite{pof_impl} came to a similar conclusion. They suggest that multiple tables can be combined into a single table. When appropriate, a user should also consider replacing a table with a conditional statement such as an if statement or match statement in Steve.