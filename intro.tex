\chapter{INTRODUCTION} \label{ch:intro}

% FIXME: The introduction needs to flow. Don't break it up into this kind of
% question and answer dialog, and don't skimp on motivation. Take this guy's
% PhD thesis as an example:
%
% http://www.cs.colostate.edu/~christos/thesis/chp1.pdf
%
% It's a little more detailed than you'll need, but it's a good example of
% how to motivate the problem and organize both the solution and your
% contributions.

% FIXME: Use \emph to emphasize terms, not boldface.

% FIXME: Use this sentence in your abstract.

The Internet is nowadays ubiquitous with everyday life. In the last twenty years,
the industry has rapidly evolved to incorporate all kinds of new Internet services:
cloud services, web services, Internet of Things, etc. As the world grows
more reliant on services provided over a network, so must the network evolve.

Modern routers and switches provide the backbone for the Internet.
These modern devices implement two fundamental components in their firmware: the
\emph{control plane} and the \emph{data plane}.
The control plane decides where packets should be sent and is capable of learning
new forwarding behaviors. The control plane commands the data plane.
The data plane is responsible for moving packets between ports.
Over time this conventional type of networking has become too rigid, unscalable, and
proprietary to support evolving technologies. 

Defining network functionality in firmware is inherently inflexible.
Firmware is architecture-dependent, difficult to write, and challenging to update
regularly.
Conventional switches are unscalable because they must deal with most or all
common protocol headers.
With dozens of common protocols now, and potentially dozens more in the future,
it becomes evident that this model cannot last forever.
Additionally, many devices are black boxes,
only exposing an interface designed by the vendor,
making them difficult to customize for enterprises with special needs.
Due to these issues and others,
a new paradigm for networking was created. 

This new paradigm
is called \textit{software-defined networking} (SDN). The Open Network
Foundation (ONF) defines SDN as ``the physical separation of the network control
plane from the forwarding [data] plane'' \cite{onf_sdn_def}.
The control plane is removed from the firmware.
Instead, it is implemented in software which typically runs on a separate machine.
This software control plane is capable of controlling several devices over a
single network using messages.
The most popular open-source standard for this type of networking is OpenFlow \cite{openflow_spec}.

This programmable control plane gives programmers the ability to
provide network functions (such as firewalls, network address translation, etc) in 
software rather than proprietary hardware.
Any programmer can write regular software for common operating
systems and processors to control the behavior of their networking device.

The programmable control plane is only the first step. The next
step is the \emph{programmable data plane}. By decoupling the data and control planes, 
it allows them both to evolve separately. A programmable data plane provides
two key benefits that the software control plane cannot: low-latency, customized packet processing and \emph{protocol independence}.

In the control and data plane relationship,
the control plane is smart but slow, while the data plane is fast yet dumb. That is,
the control plane can perform general purpose computing but communicating with
it has high latency. The data plane, on the other hand, is the fast path for packet processing.
It has very limited computing power, but can process packets at a low latency.
As programmable data planes evolve and support more flexible instruction sets,
communicating with the control plane becomes less necessary.

Protocol independence means that a networking device is not required to support
a suite of well-known protocols (ethernet, IPv4, TCP, etc) by default.
Rather than relying on specialized hardware or firmware tailored towards parsing/decoding
these protocols, the data plane supports processors
and generic instructions that a user could use to decode and operate on
any header and field.
This allows users to define their own protocols in addition to
supporting well-known protocols. This makes the data plane scalable, that is, it
can adapt to new headers easily.

In order to pursue programmable SDN devices, there must be a language to write
packet processing programs. A number of language solutions arise here.
Some attempt to solve this problem by providing APIs for existing programming languages
(specifically C/C++).
Others attempt to produce domain-specific languages (DSLs) for SDN devices.
The focus of this thesis is one such DSL, Steve, a language for the safe and
efficient specification of packet processing applications for programmable data planes.

% Packet processing and forwarding logic is usually performed using an abstract
% model known as a \emph{packet processing pipeline} which is a generalization of
% the pipeline model described by OpenFlow \cite{openflow_spec}.


%
% The programmer can customize a switch with customized packet processing
% applications and forwarding rules. Typically, this is done through an abstract
% model called a \textit{packet processing pipeline}.
% The pipeline makes decisions, performs operations, and applies forwarding rules on packets.
%
% Forwarding rules can handle any header and can be user-defined. It opens the way
% to networking applications made for custom protocols and forwarding behaviors.
% By changing the program which drives the packet processing pipeline, the user
% can \textit{change the device's behavior without changing the device's
% hardware}. The same device can go from being a repeating hub to a MAC learning
% switch to a router just by running a different program on it.

\section{The Steve Programming Language}

Steve is a protocol independent, architecture independent, strongly-typed,
declarative language for writing packet processing applications on SDN devices.
Steve allows programmers to define network functions using an abstract model
known as a \emph{packet processing pipeline} -- a generalization
of the pipeline defined by OpenFlow \cite{openflow_spec}.
A packet processing pipeline is an algorithm that
uses multiple stages to process and forward packets.
Steve provides high-level language features for specifying this pipeline.
Specifically, Steve allows for the definition of:

\textbf{Header Structures}.
A programmer may define the structure of any protocol header using an abstract
mechanism known as a \emph{layout}.
A layout specifies what fields are in a header, the lengths of those fields,
and types for those fields.

\textbf{Decoders}. Decoders are special functions for extracting fields from a
packet header. They conform to user-defined layouts, making them flexible enough
to deal with any protocol header.

%
% Steve allows the user to program special functions decoders
% which are used for decoding and extract fields from a header. Decoders allow the
% user to specify which fields they need. Decoders do not require that a user
% extract all fields if they do not need those fields. The user is not required to
% manually extract the bytes associated with a field. Instead, the user gives a
% field name from the header layout they defined, and the language generates the
% code for them.

\textbf{Flow tables}. 
In networking, a \emph{flow} is a group of packets from a source to one or more 
destinations. In order to facilitate these flows, Steve uses the
\emph{flow table} (an OpenFlow inspired abstraction). 
A flow table is a dynamic
decision table which classifies packets into groups (flows) using
decision rules (known as \emph{flow entries}).
Packets which are part of the same flow have a common set of \emph{actions}
applied to them.
Flow tables are responsible for the majority of forwarding logic.

\textbf{Actions}. Actions provide a way to modify a packet's fields, add/remove
flow entries from tables, and forward/drop packets. Steve actions are a generalization
of OpenFlow's instructions and actions.

\textbf{Pipeline Composition}. A Steve pipeline is a composition of two kinds of stages:
decoders and flow tables.
A packet moves through these stages which perform some set of processing operations.
Eventually one of these stages will make a forwarding decision for that packet.
The language provides a mechanism for connecting these
stages together in a number of flexible ways.

\textbf{Event handling}. The pipeline may raise events when it cannot handle a
packet. Event handlers may be defined to deal with events such as learning a new
flow, removing an old flow, etc.

Though Steve is an SDN language, it also supports a number of general purpose
features similar to C. Steve supports functions, variables, arithmetic, branching, looping
and a foreign function interface which may be used to call linked C functions.

\section{Motivation: Issues with Low-level Languages}

A network program must enforce safety over any other concepts.
Network programs, especially those running on high-traffic devices, must not
crash due to logical mistakes or unnoticed undefined behaviors.
Such errors could result in security holes, denial of service attacks, and
disastrous network downtime.

Typical packet processing programs are written in low-level languages, most
commonly C. These C programs usually work with data plane programming APIs such
as Open Data Plane (ODP) \cite{odp_webpage} and the Data Plane Programming Kit
(DPDK) \cite{dpdk_webpage}. Programming network functions in C can be prone
to logical error and unnoticed undefined behavior stemming from buffer management
and resource management. These issues are compounded by the fact C is a general
purpose language not tailored towards packet processing, thus the burden of safety
is left to the programmer.
Specifically, C programs open the opportunity for logical errors and undefined
behaviors such as:

\begin{itemize}
\item accessing memory outside the bounds of a packet buffer or within a constrained
region of memory,

\item writing bytes which exceed a constrained subset of the packet
resulting in a buffer overflow (i.e. writing new bytes into a header field
but exceeding the size of that field),

\item buffer underflow resulting from not writing enough bytes when modifying
a field,

\item using the value of a field in an operation that is not supported by its
range of value,

\item non-terminating cycles in program logic (an infinite loop
triggered by a single packet),

\item incorrect assumptions about decoding state. That is, a programmer using
a field which they have not extracted yet.
\end{itemize}

Programmers using C for SDN applications are also burdened with management of
device resources. Specifically, a C programmer must manage:

\begin{itemize}
\item memory. The programmer must manage their own buffers for holding packet
data and packet meta data. This opens the opportunity for accidental
memory leakage.

\item ports. The programmer is responsible for receiving, reading, and forwarding
through ports. This process should be decoupled from the packet processing logic
for modularity. A packet processing application should be architecture independent
whereas port management is a architecture dependent problem.
\end{itemize}

% Low-level packet processing code can be very light on easy to understand
% abstraction and very heavy on gritty implementation details. The programmer
% should be focusing on high-level abstractions; concepts such as: what fields are
% needed, what operations must be performed on these fields, conditional decision
% making, packet classification, etc. Instead, they are worried about the many
% common ailments of writing in low-level code.
%
% The programmer must deal with resource allocation and management. They must deal
% with allocating their own buffers for storing packets and by extension must deal
% with de-allocating those buffers. This type of resource management is prone to
% accidental memory leakage, especially in C. Memory leakage of gigabytes of data
% will quickly bring a system down.
%
% The programmer must directly manage ports. They are also responsible for
% manually receiving, reading, and sending packets on those ports. This is code is
% common to all packet processing applications. The programmer should not be
% burdened with this kind of common code when writing a packet processor.
%
% When working with packet headers, the programmer must manually work with raw
% arrays of bytes. The burden of decoding fields from byte arrays and interpreting
% the value of those fields is shifted onto the programmer. Manual decoding can be
% a painful process on its own. It is prone to error, particularly off-by-N
% errors. When reinterpreting those bytes into header data structures, the result
% can be disastrous if the bytes are wrong or the data structure is wrong. Moving
% or copying those bytes around is particularly prone to buffer overflows.
%
% A number of other common problems come up when working in low-level code.
% Undefined behavior from reading past the end of packet buffers can be one.
% Performing operations on null pointers, invalid pointers, or uninitialized
% memory is another.
%
% Steve and its runtime, Flowpath, attempt to remedy most of these problems. The
% objective is to abstract many of these low-level details. Instead, the
% programmer only focuses on the high-level abstractions. Specifically, the Steve
% language focuses the programmer on writing the packet processing pipeline and
% makes the grittier details opaque.

\section{Contribution: A Language for Defining Safe Pipelines}

The Steve language provides a type and constraints system to ensure the safety
of all Steve-defined packet operations and pipelines.
Steve targets the runtime environment, Flowpath, abstracts the resources
such as memory and ports, of a given hardware device so the Steve program does not
have to.
In addition, Steve uses an optimizing compiler to ensure efficient code generation.

\subsection{Type Safety}

Steve is a statically-typed language. The compiler performs additional work to
enforce strict safety guarantees so that runtime checks can be avoided as much as possible.

The Steve type system ensures that operations on header fields are valid and
well-defined. 
To reduce errors related to working with fields as byte buffers, Steve allows
for the representation of fields as arbitrary precision, signed or unsigned integers.
Implicit integer conversions can be applied to these fields where necessary.
Conversions prevent field modification from ever underflowing or overflowing.

Steve does not support pointer types, meaning that the programmer is never
concerned with null or invalid pointer addresses. Instead, Steve supports reference
types which are guaranteed to refer to initialized memory.

%\begin{enumerate}
%\item the representation of fields as
%arbitrary precision, signed or unsigned integers, (as long as they
%are multiples of 8). This allows the programmer to avoid using arrays of bytes
%to represent fields.
%
%\item logical and arithmetic expressions which are type checked
%to ensure no undefined behavior happens (e.g. shifting by negative values,
%adding to a boolean, etc).
%
%\item implicit conversions can be applied where necessary (e.g. integer size
%promotion, unsigned to signed conversions, etc).
%
%\item buffer overflow/underflow prevention. 
%Writing a new value into a header field \textit{never} causes accidental
%buffer overflows. The Steve compiler uses implicit conversions to guarantee that
%the size of the new value fits exactly into the byte width of a field. If the
%new value is represented in less bytes than the size of field, it gets extended
%to fit. Values that are too large get truncated.
%\end{enumerate}


% Specifically, the Steve language aims to avoid:
%
% \begin{itemize}
% \item accessing memory outside the bounds of a packet buffer,
%
% \item writing bytes which exceed a contrained subset of the packet
% resulting in a buffer overflow (i.e. writing new bytes into a header field
% but exceeding the size of that field),
%
% \item buffer underflow resulting from not writing enough bytes when modifying
% a field,
%
% \item using the value of a field in an operation that is not supported by its
% range of value,
%
% \item non-terminating cycles of decoders and table matchings (an infinite loop
% of packet processing),
%
% \item incorrect assumptions about decoding state. That is, a programmer using
% a field which they have not extracted yet.
% \end{itemize}

\subsection{Pipeline Constraints System}

The Steve compiler ensures certain guarantees about the correctness of a Steve
pipeline. Steve pipelines may be represented as directed acyclic graphs. The
Steve compiler performs analysis on this graph to enforce the following
constraints.

\begin{enumerate}
\item Fields that have not been extracted by a decoder cannot be used. 

\item The traversal of a packet through a pipeline is always a linear
progression. That is, a packet may never enter a non-terminating cycle
of decoders and tables.
\end{enumerate}

\subsection{Resource Safety}

Steve never requires the user to heap allocate resources. The majority of
allocation is done on the stack, ensuring Steve applications are free of
memory leakage and faster (as heap allocation tends to be slow).

Steve relies on its runtime system, Freeflow \cite{freeflow_software}, 
to manage system resources such as
ports, packet reading, and packet buffer allocations.
This decouples the logic of system resource management from the packet
processing logic.

\subsection{Efficiency}

%Steve's programmable decoders provide the user the chance at certain
%optimizations. Steve allows the user to specify exactly which fields from a
%header they want extracted rather than forcing them to extract the entire thing.

High-level abstractions tend to produce performance penalties in exchange
for safety. The Steve compiler tries to reduce this penalty as much as possible.
The Steve compiler generates LLVM intermediate representation (IR) code
\cite{llvm_webpage}. The LLVM IR optimizing compiler is able to produce 
code equivalent to C programs such as:
function inlining, peephole optimizations, etc.

Steve may also customize the LLVM compiler to produce
specialized instructions that are optimized for the architecture running
the application. This would be future work.

\section{Thesis Overview}

This thesis is organized as follows.

Chapter \ref{ch:pipeline_model} describes the components of Steve programs
and pipelines. It provides many of the abstract concepts a programmer must
understand about packet processing before they can begin writing a Steve
program.

Once a programmer understands the fundamentals of the pipeline processing model,
they may begin with the Steve tutorial in Chapter \ref{ch:tutorial}.
This tutorial explains how to write network applications using the
Steve programming language. Sample networking applications are also presented in
this chapter. These same samples are presented again in Appendix
\ref{ap:steve_programs}. 

Chapter \ref{ch:users_guide} serves as a reference
manual which includes semantic, grammar, and typing rules for the language.
This reference manual may be helpful to refer to when confused about a
Steve code example.
It is not to be read as a typical chapter. Instead, one should 
treat it like an annotated appendix.

Chapter \ref{ch:flowpath} describes how Steve and its runtime environment, Freeflow,
interact with each other and the underlying hardware system.

Chapter \ref{ch:experiments} provides experiments using sample Steve
applications. Pcap files are run through Steve applications to measure the data
rate and raw packet rate that each application is capable of running at on a
Linux machine.

Chapter \ref{ch:related} describes similar
works in the field of SDN and SDN programming languages. It also provides a
background into SDN and the different approaches Steve takes to other languages
of its kind.
